{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9643e337-f40a-40b9-aa65-4b76f3dcd792",
   "metadata": {},
   "source": [
    "# Text Preprocessing in NLP: A Concise Guide \n",
    "\n",
    "## Why Preprocess Text?\n",
    "\n",
    "Imagine you're a chef preparing a complex dish. You wouldn't use every ingredient in your kitchen, right? Similarly, in Natural Language Processing (NLP), we don't use raw text as-is. We carefully select and prepare our \"ingredients\" (words and features) to make our \"dish\" (model) as delicious (accurate) as possible.\n",
    "\n",
    "Let's explore this concept using spam classification as an example.\n",
    "\n",
    "### Spam Classification Example\n",
    "\n",
    "To classify emails as spam or not:\n",
    "1. Analyze both spam and non-spam emails\n",
    "2. Identify distinctive features (e.g., average length, common words)\n",
    "3. Use these features to train a model\n",
    "\n",
    "But here's the catch: emails can be messy! They might contain:\n",
    "- Emojis ðŸ˜Š\n",
    "- Special characters @#$%\n",
    "- Numbers 123\n",
    "- HTML tags <br>\n",
    "- Varying letter cases\n",
    "\n",
    "This \"noise\" can make pattern recognition challenging. That's where text preprocessing comes in!\n",
    "\n",
    "## What is Text Preprocessing?\n",
    "\n",
    "Text preprocessing is like sorting through your groceries before cooking:\n",
    "\n",
    "> ðŸ›’ **Text Preprocessing** = Picking the most useful \"ingredients\" from your raw text data\n",
    "\n",
    "It's a crucial step in NLP where we clean and transform raw text to make it more suitable for analysis.\n",
    "\n",
    "### Benefits of Text Preprocessing\n",
    "\n",
    "- Reduces complexity\n",
    "- Lowers computational cost\n",
    "- Eliminates redundancy\n",
    "- Improves model generalization\n",
    "\n",
    "## The Text Preprocessing Pipeline\n",
    "\n",
    "Here's a typical sequence of text preprocessing steps:\n",
    "\n",
    "1. **Lowercase Conversion** \n",
    "   - Why? \"Hello\" and \"hello\" are the same word\n",
    "\n",
    "2. **Tokenization** \n",
    "   - What? Breaking text into individual words or subwords\n",
    "\n",
    "3. **Punctuation Removal** \n",
    "   - Why? \"hello\" and \"hello!\" are essentially the same\n",
    "\n",
    "4. **Stopword Removal** \n",
    "   - What? Removing common words like \"the\", \"is\", \"at\"\n",
    "\n",
    "5. **Vectorization** \n",
    "   - Why? Converts text to numerical format for machine learning models\n",
    "\n",
    "Remember, this pipeline can be customized based on your specific NLP task!\n",
    "\n",
    "## Key Takeaway\n",
    "\n",
    "Text preprocessing is about cleaning and transforming raw text to extract the most relevant features for your NLP task. It's an essential step that can significantly impact your model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd699103-4a1b-4f30-a89d-811ef70ae2e0",
   "metadata": {},
   "source": [
    "# NLP Terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d471a336-7fb4-4aed-8932-8ad6a47b0d4d",
   "metadata": {},
   "source": [
    "## Corpus\n",
    "- Definition: A collection of documents in a dataset\n",
    "- Purpose: Serves as the primary data source for NLP tasks\n",
    "- Note: Plural form is \"corpora\"\n",
    "\n",
    "## Documents\n",
    "- Definition: Individual units within a corpus\n",
    "- Composition: Typically sentences or paragraphs\n",
    "- Importance: Basic units of analysis in many NLP tasks\n",
    "\n",
    "## Vocabulary\n",
    "- Definition: Set of all unique words in a corpus\n",
    "- Also known as: Lexicon or dictionary\n",
    "- Use: Fundamental for text analysis and model training\n",
    "\n",
    "## Tokens\n",
    "- Definition: Individual words or subword units\n",
    "- Process: Result of tokenization\n",
    "- Importance: Basic units for most NLP operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb90936-325b-494f-bf55-212d1614ce9f",
   "metadata": {},
   "source": [
    "# Important Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e31ee4a6-b62b-44f4-89e2-daedcf16a6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/verykul/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/verykul/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/verykul/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk \n",
    "import string \n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26863f33-f3ba-40cc-ab6a-9feaf8daac07",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Upper to Lower case conversion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e635e16-2af2-4d78-a9ae-50e0cc5467d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Natural language processing (NLP) is an interdisciplinary subfield of computer science and artificial intelligence. \n",
    "It is primarily concerned with providing computers the ability to process data encoded in natural language and is thus closely related to information retrieval, knowledge representation and computational linguistics, a subfield of linguistics. \n",
    "Typically data is collected in text corpora, using either rule-based, statistical or neural-based approaches of machine learning and deep learning.\n",
    "Major tasks in natural language processing are speech recognition, text classification, natural-language understanding, and natural-language generation. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4d09e4c-6a94-4a86-8dd0-da19eebe8978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "natural language processing (nlp) is an interdisciplinary subfield of computer science and artificial intelligence. \n",
      "it is primarily concerned with providing computers the ability to process data encoded in natural language and is thus closely related to information retrieval, knowledge representation and computational linguistics, a subfield of linguistics. \n",
      "typically data is collected in text corpora, using either rule-based, statistical or neural-based approaches of machine learning and deep learning.\n",
      "major tasks in natural language processing are speech recognition, text classification, natural-language understanding, and natural-language generation. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = text.lower()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653c4dcf-0932-4bdd-acc9-bcdf65e4c7b1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Regular Expression "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed0243d-b30a-474b-b202-580c650c17d8",
   "metadata": {},
   "source": [
    "> Regular expression can be used to remove any unwanted pattern from our texts. It is most commonly used in natural language tasks to remove html tags, special characters and other non desired patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "836123ec-3359-4794-886c-a984267b7b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original_text: Hello, World! Welcome to Python programming: the best & most powerful language.\n",
      "cleaned_text: Hello World Welcome to Python programming the best  most powerful language\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, World! Welcome to Python programming: the best & most powerful language.\"\n",
    "\n",
    "pattern = r'[^a-zA-Z0-9s\\s]'  #matches any character that is not letter, digit or white space \n",
    "\n",
    "cleaned_text = re.sub(pattern, '', text)\n",
    "print('original_text:', text)\n",
    "print('cleaned_text:',cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1df6e20-ac88-4155-acbd-f0fd089dfa95",
   "metadata": {},
   "source": [
    "> We can also remove html tags from our text using Regular Expression as well "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73b2d6c6-ec5c-495c-9aad-fad3d93859e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text: <html><body><h1>Hello, World!</h1><p>This is a paragraph.</p></body></html>\n",
      "cleaned text: Hello, World!This is a paragraph.\n"
     ]
    }
   ],
   "source": [
    "text = \"<html><body><h1>Hello, World!</h1><p>This is a paragraph.</p></body></html>\"\n",
    "\n",
    "pattern = r'<[^>]+>'\n",
    "cleaned_text = re.sub(pattern, '', text)\n",
    "print('original text:', text)\n",
    "print('cleaned text:', cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b9cf3c-e0ca-43f1-a071-af1bd15e1325",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Punctuation Removal "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0ff4f1-6af6-48fe-ae18-a2bd30ad09c6",
   "metadata": {},
   "source": [
    "\n",
    "- As the name suggests we remove punctuation from the documents. But, you may ask why we remove them ? Well we could keep them if we want and they are kept for some applications of NLP. But,<br>\n",
    "\n",
    "- **In the context of prediction and finding patterns. Example being - Spam classifier, Fake news classifier etc. For tasks like these punctuations contribute so less to the overall decision that they are literally redundent and in return they add noise to our model.** <br>\n",
    "\n",
    "- Therefore it is just better to get rid of them for tasks like these. But, for tasks like Natural Language Generation, Machine Translation they are kept so the model could understand grammar and produce grammatical correct texts.\n",
    "\n",
    "For now this is how you can remove punctuation from documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21047769-b096-43c4-866e-581b9b649168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21784bde-12cd-4dae-94e5-2ecf88c4f5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is ! $$punctutation removal$$, using the string library.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "255998d0-d147-426c-801c-d65cf2946128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is  punctutation removal using the string library\n"
     ]
    }
   ],
   "source": [
    "cleaned_text = [word for word in text if not word in string.punctuation]\n",
    "print(''.join(cleaned_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5650e785-33d5-42ad-af96-0430a16769fa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c2e9f4-3798-412c-ae2a-aa6523ac97d3",
   "metadata": {},
   "source": [
    "- Tokenization is the process of breaking text into smaller units called tokens. These tokens are typically words, numbers, or punctuation marks.\n",
    "\n",
    "- **Types of tokenization**:\n",
    "  1. Word tokenization: Splits text into individual words\n",
    "  2. Sentence tokenization: Divides text into sentences\n",
    "  3. Subword tokenization (e.g., BPE, WordPiece)\n",
    "  4. Language-specific tokenizers for non-English texts\n",
    "\n",
    "- **Implementation methods**:\n",
    "  - NLTK library: Offers both word and sentence tokenization\n",
    "  - Python's built-in `split()` function: A simple way to tokenize based on whitespace\n",
    "\n",
    "- **Importance**:\n",
    "  - Fundamental step in text preprocessing\n",
    "  - Enables further analysis and feature extraction\n",
    "  - Crucial for tasks like part-of-speech tagging and named entity recognition\n",
    "\n",
    "- **Challenges**:\n",
    "  - Handling contractions (e.g., \"don't\")\n",
    "  - Dealing with hyphenated words\n",
    "  - Managing special characters and punctuation\n",
    "\n",
    "Remember: The choice of tokenization method can significantly impact downstream NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550b15da-985e-4563-9aaf-aa8ab9d93f07",
   "metadata": {},
   "source": [
    "`Example - 1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2609dde-628a-4fda-8a09-210084ef00a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'how', 'tokenization', 'is', 'done', 'to', 'word', '.', 'This', 'is', 'tokenizatio', 'using', 'NLTK']\n",
      "['This is how tokenization is done to word.', 'This is tokenizatio using NLTK']\n"
     ]
    }
   ],
   "source": [
    "text = \"This is how tokenization is done to word. This is tokenizatio using NLTK\"\n",
    "\n",
    "# word tokenization \n",
    "print(word_tokenize(text))   # output is a list of words\n",
    "print(sent_tokenize(text))   # output is a list of sentences "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100a483b-d1d6-4764-95ab-1dda3d7fb593",
   "metadata": {},
   "source": [
    "`Example - 2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "664e9e27-8d3f-4809-bd5a-5de0669f4bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'email', 'is', 'not', 'valid', ':', 'example', '@', 'gmail.com']\n"
     ]
    }
   ],
   "source": [
    "text = \"This email is not valid: example@gmail.com\"\n",
    "\n",
    "print(word_tokenize(text))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6744e7f1-1583-4700-8027-55caafcaf38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'email', 'is', 'not', 'valid:', 'example@gmail.com']\n"
     ]
    }
   ],
   "source": [
    "print(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9afc03-cca9-4319-982d-012c28f47ff6",
   "metadata": {},
   "source": [
    "- In the second example you can see the NLTK word tokenizer converts the email to three different tokens'example', '@' and 'gmail.com'. while the split function just returned the whole email as a token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee33eff-f382-431d-9653-6d087b8f776b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a4d4a0-eba6-4184-8fe5-f83b29936802",
   "metadata": {},
   "source": [
    "- Stopwords also called Function words are words that are mostly use to make sentences grammatically correct. They do not necessarily contribute to the true meaning of the sentence and removing them does not affects the overall context of the sentence.\n",
    "- NLTK library already has a stopwords API which returns a list of all the stopwords for different languages. Here is an example for the stopwords in English and how you can remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e60b83d-f5cd-4b19-b4eb-12a9a3817491",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"NLTK, or the Natural Language Toolkit, is a treasure trove of a library for text preprocessing. Itâ€™s one of my favorite Python libraries. NLTK has a list of stopwords stored in 16 different languages\"\n",
    "text_tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09e42d10-0a5c-4785-b4b8-bc0c641d0a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = stopwords.words('english')\n",
    "# print(STOPWORDS)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c05383c5-32ee-413b-a312-dfae74184eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text after removal of stopwords: \n",
      "\n",
      "NLTK , Natural Language Toolkit , treasure trove library text preprocessing . It â€™ one favorite Python libraries . NLTK list stopwords stored 16 different languages\n"
     ]
    }
   ],
   "source": [
    "filtered_sentence = [word for word in text_tokens if not word in STOPWORDS]\n",
    "print('The text after removal of stopwords: \\n')\n",
    "print(' '.join(filtered_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835d1404-2968-4021-9f73-b4de462b2334",
   "metadata": {},
   "source": [
    "- Well the output is different from what we gave but you can see that the meaning of the sentence is not lost. We can still somehow manage to understand the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7eda0ba-77b3-4a88-9884-427a45feda04",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Stemming "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3755f65-32c5-4878-b2a0-7fbcfa7b738a",
   "metadata": {},
   "source": [
    "- Stemming is essentially just breaking a complex word into its base or root form. To understand stemming it's better if we see the example first - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bfd1db8-4048-4bc0-92cc-430f452c5127",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "475e4aa7-bf6b-4e16-978d-2f635486941e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"She was a great Dancer. Her performance was historical\"\n",
    "tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe10db18-db68-43bc-87a8-e307870d2704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['she', 'wa', 'a', 'great', 'dancer', '.', 'her', 'perform', 'wa', 'histor']\n"
     ]
    }
   ],
   "source": [
    "stemmed_words = [stemmer.stem(word) for word in tokens]\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471ffd23-352b-4a56-963d-c5afdb317239",
   "metadata": {},
   "source": [
    "- You can see that the resulting words are broken into their base form but some of them are not from the english dictionary, they are not a real word of english. This is limitation of stemming, they not necessarily result in a meaning word and it ocuurs more than often.\n",
    "- Then why is it used you may ask ?\n",
    "    - well stemming is used because it is faster than other approaches. So if the corpus is large and we want faster conversion we can\n",
    "      consider using stemming\n",
    "    - Another scenario of using stemming is when we don't care about the true meaning of the word and an approximate word of the true word\n",
    "      will suffice for our use case.\n",
    "\n",
    "- Application of stemming:\n",
    "    1. Spam classification\n",
    "    2. Sentimental analysis\n",
    "    3. duplicate matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5618830-2859-4727-a3d4-acf5c974cbe1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Lemmatization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8e79c2-50c8-4f8c-bf28-2735c0bcb489",
   "metadata": {},
   "source": [
    "- Lemmatization is essentially stemming but the catch here is, it results in a meaningful word. That's the only difference between them.\n",
    "- Since it results in a meaningful word it tends to be slower than stemming.\n",
    "- Therefore it is only preferred when we want to conserve the true word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d18d092-5931-445f-8ede-cc338e18f92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0438e58-c48e-4889-ab30-81c859a2fde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"He was going to the castle. To steal the magical computer\"\n",
    "tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5db22dbf-5f14-4fe7-9316-caa31c0a7ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'wa', 'going', 'to', 'the', 'castle', '.', 'To', 'steal', 'the', 'magical', 'computer']\n"
     ]
    }
   ],
   "source": [
    "lemmatized_word = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "print(lemmatized_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eeae97-b4f9-4e67-847f-6fa9bfc898c9",
   "metadata": {},
   "source": [
    "- But wait, why is 'was' converted to 'wa', that is not meaningful at all.\n",
    "- Also there is no change in the words of the sentence as well they are not converted to their root form.\n",
    "- Well there are some implications here as well. To understand why this happens we have to know how the nltk lemmatizer works.\n",
    "  Lemmatization is a process of reducing words to their base or dictionary form, known as the lemma. It works as follows:\n",
    "\n",
    "> Lemmatization removes inflectional endings from words. Inflections are changes in word forms that indicate grammatical functions, such as\n",
    "  plurals for nouns or tense for verbs.\n",
    "\n",
    "1. The lemmatizer analyzes the word's context and part of speech to determine its proper base form.\n",
    "2. It then checks if the resulting word exists in a dictionary. If found, it returns this base form (lemma).\n",
    "3. Lemmatization considers the word's meaning and context, unlike stemming, which simply truncates words based on rules.\n",
    "4. The goal is to return a valid dictionary word that preserves the word's core meaning.\n",
    "\n",
    "- Lemmatizer works only when the a parameter `pos` is given to it. By default the parameter is set to `pos='n'`, which means **NOUN**. Hence when we called lemmatizer in our example it assumed all the text as **NOUN** and converted them into base form which are the same as original. \n",
    "Similary, for 'was' it assummed that 'was' is a noun and is a plural for 'wa' and removed the last letter 's' from the word 'was'.\n",
    "- Now when we provide the pos value and call lemmatizer again on our text we will get the base form of the word as verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6dfab94f-e2f8-4be4-ab7d-ae1facee1488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'be', 'go', 'to', 'the', 'castle', '.', 'To', 'steal', 'the', 'magical', 'computer']\n"
     ]
    }
   ],
   "source": [
    "lemmatized_word = [lemmatizer.lemmatize(word, pos='v') for word in tokens]\n",
    "print(lemmatized_word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
